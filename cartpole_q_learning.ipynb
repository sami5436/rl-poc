{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Reinforcement Learning\n",
    "This notebook demonstrates a step-by-step Q-learning approach to solve the classic CartPole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "In this section we import required libraries and create the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Provides numerical functions and array support.\n",
    "import gymnasium as gym  # Supplies the CartPole environment and other reinforcement learning tools.\n",
    "from collections import defaultdict  # Allows dictionaries with default values for new keys.\n",
    "env = gym.make(\"CartPole-v1\")  # Instantiate the CartPole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Agent\n",
    "Here we create a simple Q-learning agent that discretizes the continuous state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, buckets=(6, 12, 6, 12), alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.env = env  # Save the environment for access to action and observation spaces.\n",
    "        self.buckets = buckets  # Define how finely to discretize each observation dimension.\n",
    "        self.alpha = alpha  # Learning rate controls how much new information overrides old.\n",
    "        self.gamma = gamma  # Discount factor balances immediate and future rewards.\n",
    "        self.epsilon = epsilon  # Probability of choosing a random action (exploration).\n",
    "        self.epsilon_decay = epsilon_decay  # Multiplicative factor to decrease exploration over time.\n",
    "        self.epsilon_min = epsilon_min  # Lower bound on exploration probability.\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.env.action_space.n))  # Q-table mapping states to action values.\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        upper_bounds = self.env.observation_space.high  # Maximum values for each observation component.\n",
    "        lower_bounds = self.env.observation_space.low  # Minimum values for each observation component.\n",
    "        ratios = (obs - lower_bounds) / (upper_bounds - lower_bounds)  # Normalize observations to 0-1.\n",
    "        new_obs = [int(np.clip(ratios[i] * (self.buckets[i] - 1), 0, self.buckets[i] - 1)) for i in range(len(obs))]  # Map to discrete bins.\n",
    "        return tuple(new_obs)  # Return as a tuple so it can be used as a dictionary key.\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() < self.epsilon:  # Decide whether to explore.\n",
    "            return self.env.action_space.sample()  # Random action for exploration.\n",
    "        return int(np.argmax(self.Q[state]))  # Best known action for exploitation.\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        best_next_action = np.argmax(self.Q[next_state])  # Value of the best next action.\n",
    "        td_target = reward + self.gamma * self.Q[next_state][best_next_action] * (1 - done)  # Bellman target.\n",
    "        td_delta = td_target - self.Q[state][action]  # Temporal difference error.\n",
    "        self.Q[state][action] += self.alpha * td_delta  # Update Q-value.\n",
    "        if done:  # After each episode,\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)  # Decay exploration rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Loop\n",
    "We repeatedly interact with the environment so the agent can learn good actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(env)  # Create an instance of our Q-learning agent.\n",
    "n_episodes = 500  # Total number of episodes to train.\n",
    "for episode in range(n_episodes):  # Loop over each episode.\n",
    "    obs, _ = env.reset()  # Reset environment and get initial observation.\n",
    "    state = agent.discretize(obs)  # Convert observation to discrete state.\n",
    "    done = False  # Track whether the episode has ended.\n",
    "    total_reward = 0  # Sum of rewards for this episode.\n",
    "    while not done:  # Continue until episode finishes.\n",
    "        action = agent.choose_action(state)  # Pick action based on policy.\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)  # Apply action to environment.\n",
    "        done = terminated or truncated  # Determine if episode is over.\n",
    "        next_state = agent.discretize(next_obs)  # Discretize the next observation.\n",
    "        agent.learn(state, action, reward, next_state, done)  # Update agent with observed transition.\n",
    "        state = next_state  # Move to the next state.\n",
    "        total_reward += reward  # Accumulate episode reward.\n",
    "    if (episode + 1) % 50 == 0:  # Every 50 episodes,\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")  # Output progress information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "After training, we run one episode without exploration to evaluate the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()  # Reset environment for evaluation.\n",
    "state = agent.discretize(obs)  # Discretize starting observation.\n",
    "done = False  # Track episode completion.\n",
    "total_reward = 0  # Total reward accumulated during evaluation.\n",
    "while not done:  # Run until the episode ends.\n",
    "    action = int(np.argmax(agent.Q[state]))  # Select the best known action without exploration.\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)  # Apply the action.\n",
    "    state = agent.discretize(obs)  # Update the current state.\n",
    "    done = terminated or truncated  # Check if the episode has finished.\n",
    "    total_reward += reward  # Update the cumulative reward.\n",
    "print(f\"Evaluation reward: {total_reward}\")  # Display the total reward from the evaluation episode.\n",
    "env.close()  # Close the environment to free resources.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
